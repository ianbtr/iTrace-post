{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Raw Data From iTrace for Experiments Containing Edits\n",
    "\n",
    "This procedure requires that you recorded your experiment session with [FLUORITE](http://www.cs.cmu.edu/~fluorite/) in addition to iTrace.\n",
    "\n",
    "In iTrace v0.0.1 (Alpha), editing is not supported. *If you used a later version of iTrace that explicitly supports editing, this issue does not apply to your experiment, and you do not need the code in this example.* The work-around presented here is very time-consuming and should be avoided if at all possible.\n",
    "\n",
    "Broadly, this repository combines the capabilities of iTrace and FLUORITE to create functionality that supports editing during an experiment. The `analyzer.py` script is an example of how to use this repository to create a post-processing workflow. In this notebook, that script will be explained in greater detail such that you can write one of your own.\n",
    "\n",
    "## Contents:\n",
    "* [General Procedure](#General-Procedure)\n",
    "* [Ingesting the Raw Data](#Ingesting-the-Raw-Data)\n",
    "* [Partitioning Raw Data](#Partitioning-Raw-Data)\n",
    "* [Running gaze2src](#Running-gaze2src)\n",
    "* [Additional Analyses](#Additional-Analyses)\n",
    "* [Accumulating Segmented Data](#Accumulating-Segmented-Data)\n",
    "* [Appendix: Tracing Code Regions](#Appendix:-Tracing-Code-Regions)\n",
    "\n",
    "## General Procedure:\n",
    "<img src=\"../img/chart.png\" alt=\"chart\" width=\"500\"/>\n",
    "Three (3) files are required to run iTrace-post:\n",
    "* A log from iTrace-Core of the form `core*.xml`\n",
    "* A log from the iTrace plugin, of the form `eclipse*.xml`\n",
    "* A log from the FLUORITE plugin, of the form `log*.xml`\n",
    "\n",
    "All of these must be present to use the functionality described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from subprocess import DEVNULL\n",
    "from fluorite import ProjectHistory, GazeDataPartition\n",
    "from itrace_post import post_to_aoi, create_combined_archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingesting the Raw Data\n",
    "\n",
    "The log from FLUORITE should first be parsed. This creates a `fluorite.ProjectHistory` object that stores information about edits, and can retrieve the content of any file at any time on demand. To use this functionality by itself, please refer to the [relevant notebook](fluorite.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# A log file from FLUORITE is parsed\n",
    "fluorite_log_location = \"sample-data/log-files/Log-sample.xml\"\n",
    "project_history = ProjectHistory(fluorite_log_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An aside: for your analysis, you may wish to trace the locations of functions and other important sections of code so that fixations can be mapped to these regions. This is the mode used by `analyzer.py`. This feature is further explained in the [Appendix](#Appendix:-Tracing-Code-Regions).\n",
    "\n",
    "Next, ingest the iTrace log file from Eclipse. Note that iTrace may not record times in UTC, but FLUORITE does. One way to correct this is to ensure that your machine's system time is set to UTC when recording, but it may be better to deduce the correct offset (most likely an integer number of hours) and specify it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In our timezone at least, the time iTrace records is four hours \n",
    "#    behind that of FLUORITE.\n",
    "time_offset = 4*3600*1000\n",
    "eclipse_log = \"sample-data/log-files/eclipse_log.xml\"\n",
    "data_partition = GazeDataPartition(eclipse_log, time_offset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning Raw Data\n",
    "\n",
    "We can now create a timeline of project files on the same range of times found in the iTrace plugin log. Then we can save chunks of the plugin log to the same location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a file timeline. This creates files in the specified directory, \n",
    "#    and returns a list of time ranges.\n",
    "output_dir = \"sample-output\"\n",
    "\n",
    "time_periods = project_history.save_timeline(\n",
    "    output_dir,         # Directory in which to create timeline\n",
    "    granularity=\"finest\",    # Granularity (can also be set to create uniform time periods)\n",
    "    first_time=data_partition.first_time,\n",
    "    last_time=data_partition.last_time\n",
    ")\n",
    "\n",
    "# Write chunks of the plugin log to this directory\n",
    "data_partition.create_partition(time_periods=time_periods)\n",
    "data_partition.save_partition(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running gaze2src\n",
    "\n",
    "Now we have all our raw data partitioned such that each segment represents a period with no changes. This means we're almost ready to run the `gaze2src` program that ships with v0.0.1 of iTrace. Before this, however, we need to create gzip and srcml archives of all the code we just generated.\n",
    "\n",
    "This is the time-consuming part. You'll need to wait a while for this to terminate.\n",
    "\n",
    "For convenience, the sample here has been truncated to just a single segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t0_1562859140262-1562859424707\n"
     ]
    }
   ],
   "source": [
    "# Parameters for gaze2src:\n",
    "FILTER = \"ivt\"\n",
    "FILTER_ARGS = [\"-v 30\", \"-u 60\"]\n",
    "\n",
    "# Location of log file from iTrace-core\n",
    "core_log = \"sample-data/log-files/core_log.xml\"\n",
    "\n",
    "# Iterate over all segments\n",
    "for sub_dir in sorted(os.listdir(output_dir), key=lambda x: x.split(\"_\")[0]):\n",
    "    prefix = output_dir + \"/\" + sub_dir\n",
    "    \n",
    "    # Skip segments where there is no plugin log.\n",
    "    #    This can happen if edits are made very close together,\n",
    "    #    or if the plugin log terminates before the FLUORITE log.\n",
    "    if not os.path.exists(prefix+\"/plugin_log.xml\"):\n",
    "        continue\n",
    "        \n",
    "    print(\"\\t\"+sub_dir)\n",
    "        \n",
    "    # Create a tarball of code files\n",
    "    subprocess.run([\"tar\", \"-czf\", prefix+\"/src.tar.gz\", prefix+\"/code_files\"],\n",
    "                   stdout=DEVNULL, stderr=DEVNULL)\n",
    "\n",
    "    # Run srcml\n",
    "    subprocess.run([\"srcml\", prefix+\"/src.tar.gz\", \"-o\", prefix+\"/src.xml\"],\n",
    "                   stdout=DEVNULL, stderr=DEVNULL)\n",
    "\n",
    "    # Run gaze2src\n",
    "    subprocess.run([\"gaze2src\", core_log, prefix+\"/plugin_log.xml\", prefix+\"/src.xml\",\n",
    "                    \"-f\", FILTER]+FILTER_ARGS+[\"-o\", prefix+\"/gaze2src\"],\n",
    "                   stdout=DEVNULL, stderr=DEVNULL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Analyses\n",
    "\n",
    "In addition to the first-level analysis from `gaze2src`, you may generate AOIs for each segment. These are inferred from the gaze data in each segment, and are calculated using Gaussian smoothing and thresholding.\n",
    "\n",
    "If you wish to use this analysis on an experiment where iTrace was not used, or with a later version of iTrace, please refer to the [relevant notebook](aoi_generation.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over all segments\n",
    "for sub_dir in sorted(os.listdir(output_dir), key=lambda x: x.split(\"_\")[0]):\n",
    "    prefix = output_dir + \"/\" + sub_dir\n",
    "    itrace_prefix = prefix + \"/gaze2src\"\n",
    "    \n",
    "    # Check for a TSV file from gaze2src\n",
    "    try:\n",
    "        fixations_tsv = glob.glob(itrace_prefix + \"/fixations*.tsv\")[0]\n",
    "    except IndexError:\n",
    "        continue\n",
    "        \n",
    "    # The values stored in the database are also of interest.\n",
    "    fixations_db = glob.glob(itrace_prefix + \"/rawgazes*.db3\")[0]\n",
    "    \n",
    "    post_to_aoi(\n",
    "        fixations_db,             # Path to database file\n",
    "        fixations_tsv,            # Path to TSV file\n",
    "        prefix+\"/code_files\",     # Directory containing code files for this segement\n",
    "        prefix+\"/post2aoi\",       # Output directory\n",
    "        5.0,                      # Smoothing parameter\n",
    "        0.01,                     # Threshold parameter\n",
    "        time_offset=time_offset,  # Time offset\n",
    "        compute_aois=True         # Directive to compute AOIs \n",
    "                                  #     (if unset, this function just \n",
    "                                  #      converts data to a CSV)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accumulating Segmented Data\n",
    "\n",
    "Now that all the data has been processed, we can accumulate it into a single file. In this example, we have created several files of the form `*.java_AOI.csv` that list fixations on a particular code file and give their AOI number. This AOI number is mapped to a position by files of the form `*.java_AOI.json`. Because we are combining data from different segments, and different segments have differernt AOIs, information about AOIs will be lost if you do not assign each row a segment number. That is not done in this example, because only one segment is generated and analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all CSV files\n",
    "all_csvs = glob.glob(output_dir + \"/*/post2aoi/*.java_AOI.csv\")\n",
    "\n",
    "# Parse all CSV files\n",
    "archives = [pd.read_csv(\n",
    "                csv_file, parse_dates=[\"fix_time\"], index_col=[\"fix_time\"]\n",
    "            ) for csv_file in all_csvs]\n",
    "\n",
    "# Concatenate data\n",
    "all_data = pd.concat(archives, ignore_index=False)\n",
    "all_data = all_data.sort_index()\n",
    "\n",
    "# Save data\n",
    "all_data.to_csv(\"sample-output/sample_accumulated.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Tracing Code Regions\n",
    "\n",
    "You may also wish to trace the positions of particular code entities, such as functions and enums, as they are edited. This will allow you to map fixations to these regions as well as AOIs. \n",
    "\n",
    "One or more files that provide information about the starting locations of these entities are required:\n",
    "\n",
    "* A \"function index\", which is a JSON file of the following form:\n",
    "\n",
    "    ```\n",
    "    {\n",
    "        \"(Name of file without extension)\": \n",
    "        {\n",
    "           \"(Same Name.Function)\": \n",
    "               [\n",
    "                   (Starting line),\n",
    "                   (Ending line)\n",
    "               ],\n",
    "           ...\n",
    "        },\n",
    "        ...\n",
    "    }\n",
    "    ```\n",
    "    \n",
    "    This allows a fixation to be mapped to a function or other similar structure. Note that there is no requirement that the keys are functions. This file can represent any area of code.\n",
    "    \n",
    "    \n",
    "    \n",
    "* An \"entity index\", which is a similar JSON file:\n",
    "    \n",
    "    ```\n",
    "    {\n",
    "        \"(Name of file without extension)\":\n",
    "        {\n",
    "            \"(Entity Type)\":\n",
    "            {\n",
    "                \"(Entity Name)\":\n",
    "                [\n",
    "                    (Starting line),\n",
    "                    (Ending line)\n",
    "                ],\n",
    "                ...\n",
    "            },\n",
    "            ...\n",
    "        },\n",
    "        ...\n",
    "    }\n",
    "    ```\n",
    "    \n",
    "    This allows a fixation to be mapped to a particular type of code structure. The 'Entity Name' will not appear in data files, but its presence may make it easier to compose this file, as you may write a comment that indicates what text is contained in the specified range.\n",
    "\n",
    "The `analyzer.py` script is a good example of how to use this functionality. The paths to a function index and entity index are given when constructing a `ProjectHistory` object, which allows the object to compute the positions of the given regions as they change over the course of the project timeline. Saving the timeline causes copies of these files to be written alongside the code files, so that they can be referenced when calling `post_to_aoi`. This results in the classifications being appended to each row of the resulting CSVs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
